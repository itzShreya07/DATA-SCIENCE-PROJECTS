{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction\n",
    "\n",
    "## Introduction\n",
    "Customer churn is when a customer stops using a company's service. For businesses, churn leads to revenue loss, so predicting churn is very important. By identifying customers who are likely to leave, companies can take actions (discounts, offers, better service) to retain them.\n",
    "\n",
    "In this project, we aim to **predict customer churn** using machine learning techniques. We are using the **Kaggle Telco Customer Churn dataset**. This dataset contains customer details such as demographics, account information, and service usage. The target variable is `Churn` (Yes/No).\n",
    "\n",
    "### Objectives:\n",
    "1. Perform Exploratory Data Analysis (EDA) to understand patterns in churn.  \n",
    "2. Preprocess the data (handle missing values, encode categorical variables, scale numerical features).  \n",
    "3. Build machine learning models to predict churn.  \n",
    "4. Evaluate the models using metrics such as Accuracy, Precision, Recall, F1-score, and ROC-AUC.  \n",
    "5. Provide insights and possible business recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-26T11:22:09.011422Z",
     "iopub.status.busy": "2025-07-26T11:22:09.011107Z",
     "iopub.status.idle": "2025-07-26T11:22:11.560755Z",
     "shell.execute_reply": "2025-07-26T11:22:11.559596Z",
     "shell.execute_reply.started": "2025-07-26T11:22:09.011391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/customer-churn-ng-intern/sample_submission.csv\n",
      "/kaggle/input/customer-churn-ng-intern/train.csv\n",
      "/kaggle/input/customer-churn-ng-intern/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Make plots look nice\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ Load the dataset\n",
    "train = pd.read_csv(\"/kaggle/input/playground-series-s4e1/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/playground-series-s4e1/test.csv\")\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "# Quick look\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Basic EDA\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\\n\", train.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Target distribution\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(train[\"Exited\"].value_counts(normalize=True))\n",
    "\n",
    "# Quick statistical summary\n",
    "train.describe().T.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Target distribution plot\n",
    "sns.countplot(data=train, x=\"Exited\")\n",
    "plt.title(\"Target Distribution (Exited)\")\n",
    "plt.show()\n",
    "\n",
    "# üìä Correlation heatmap (numeric features only)\n",
    "numeric_features = train.select_dtypes(include=[np.number]).drop(columns=[\"Exited\", \"id\"], errors=\"ignore\")\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(numeric_features.corr(), annot=False, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Feature Engineering & Preprocessing\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Separate features & target\n",
    "X = train.drop(columns=[\"Exited\", \"id\"])   # remove target + id\n",
    "y = train[\"Exited\"]\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "# Initialize encoders & scaler\n",
    "target_enc = TargetEncoder(cols=categorical_cols, smoothing=0.2)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit-transform categorical features with TargetEncoder\n",
    "X_cat_encoded = target_enc.fit_transform(X[categorical_cols], y)\n",
    "\n",
    "# Scale numeric features\n",
    "X_num_scaled = scaler.fit_transform(X[numeric_cols])\n",
    "\n",
    "# Combine processed features into one DataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_processed = np.hstack([X_num_scaled, X_cat_encoded])\n",
    "X_processed = pd.DataFrame(X_processed, columns=numeric_cols + categorical_cols)\n",
    "\n",
    "print(\"Processed dataset shape:\", X_processed.shape)\n",
    "X_processed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° XGBoost Model Setup\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define model with tuned hyperparameters\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=10000,       # large, will stop early\n",
    "    learning_rate=0.01,       # small LR for better convergence\n",
    "    max_depth=6,              # depth of trees\n",
    "    subsample=0.8,            # row sampling\n",
    "    colsample_bytree=0.8,     # feature sampling\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",       # fast & memory efficient\n",
    "    scale_pos_weight=(y.value_counts()[0] / y.value_counts()[1]) # handle imbalance\n",
    ")\n",
    "\n",
    "print(\"‚úÖ XGBoost model initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° Cross-validation with StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Setup Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"üîπ Fold {fold+1}\")\n",
    "    \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Preprocessing + model pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', clone(xgb_model))\n",
    "    ])\n",
    "    \n",
    "    # Fit with early stopping\n",
    "    pipeline.named_steps['model'].fit(\n",
    "        pipeline.named_steps['preprocessor'].fit_transform(X_train, y_train),\n",
    "        y_train,\n",
    "        eval_set=[(\n",
    "            pipeline.named_steps['preprocessor'].transform(X_val), y_val\n",
    "        )],\n",
    "        early_stopping_rounds=200,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_val_pred = pipeline.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # ROC AUC\n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "    cv_scores.append(score)\n",
    "    \n",
    "    print(f\"‚úÖ Fold {fold+1} AUC: {score:.5f}\")\n",
    "\n",
    "print(\"\\nüìä Mean CV AUC:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° Train final model on full training data and generate submission\n",
    "\n",
    "# Refit preprocessing on full train data\n",
    "X_full = preprocessor.fit_transform(X, y)\n",
    "\n",
    "# Refit XGBoost on full train data\n",
    "final_model = XGBClassifier(\n",
    "    **params,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"auc\"\n",
    ")\n",
    "\n",
    "final_model.fit(X_full, y)\n",
    "\n",
    "# Transform test set\n",
    "X_test_transformed = preprocessor.transform(test)\n",
    "\n",
    "# Predictions (probabilities for class=1)\n",
    "test_pred = final_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# Build submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],      # Kaggle requires \"id\" column from test set\n",
    "    \"Exited\": test_pred    # Target column name in dataset\n",
    "})\n",
    "\n",
    "# Save CSV\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Submission file saved as submission.csv\")\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Feature Importance from XGBoost\n",
    "\n",
    "importances = final_model.feature_importances_\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "# Numeric + Encoded categorical\n",
    "num_features = numeric_features\n",
    "cat_features = categorical_features\n",
    "\n",
    "all_features = num_features + cat_features\n",
    "\n",
    "# Map importance scores\n",
    "feat_importances = pd.DataFrame({\n",
    "    \"Feature\": all_features,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot Top 15\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feat_importances.head(15), x=\"Importance\", y=\"Feature\", palette=\"viridis\")\n",
    "plt.title(\"Top 15 Important Features (XGBoost)\")\n",
    "plt.show()\n",
    "\n",
    "feat_importances.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Final Report\n",
    "\n",
    "print(\" Model Report :\")\n",
    "print(f\"Cross-Validation ROC AUC (mean ¬± std): {cv_scores.mean():.5f} ¬± {cv_scores.std():.5f}\")\n",
    "print(f\"Final Validation ROC AUC: {final_val_auc:.5f}\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"Public LB score will be visible after submission on Kaggle.\")\n",
    "print(\"Best CV vs Public LB comparison will guide further tuning.\")\n",
    "print(\"==================================\")\n",
    "\n",
    "# üèÜ Model Summary Notes\n",
    "report_notes = {\n",
    "    \"Model\": \"XGBoost with Target Encoding\",\n",
    "    \"Feature Engineering\": \"Categorical TargetEncoder + Normalized numeric features\",\n",
    "    \"Regularization\": \"Early stopping + tuned learning rate, max_depth, subsample, colsample_bytree\",\n",
    "    \"Evaluation\": \"Stratified 5-fold CV (ROC AUC)\",\n",
    "    \"Explainability\": \"Feature importance (XGBoost) + SHAP values\",\n",
    "    \"Next Steps\": [\n",
    "        \"Try LightGBM / CatBoost for comparison\",\n",
    "        \"Hyperparameter tuning with Optuna\",\n",
    "        \"Stacking ensemble (XGBoost + LightGBM + Logistic Regression)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(report_notes)\n",
    "\n",
    "# ‚úÖ Reminder for Kaggle submission\n",
    "print(\"\\n‚û°Ô∏è Now upload 'submission.csv' to Kaggle and track Public LB score!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° Hyperparameter Tuning with Optuna\n",
    "\n",
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 2000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.2, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 10),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1, 20),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"hist\"\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"roc_auc\", n_jobs=-1)\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)  # üî• you can increase to 200+ for stronger tuning\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best CV ROC AUC:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with tuned params\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"tree_method\": \"hist\"\n",
    "})\n",
    "\n",
    "final_model = XGBClassifier(**best_params)\n",
    "final_model.fit(X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False)\n",
    "\n",
    "# Predict on test\n",
    "y_test_pred = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Save submission\n",
    "submission = pd.DataFrame({\"id\": test_df[\"id\"], \"Exited\": y_test_pred})\n",
    "submission.to_csv(\"submission_optuna.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ submission_optuna.csv is ready ‚Äì upload to Kaggle!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öñÔ∏è Train/Validation Split with Stratified K-Fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define number of folds\n",
    "n_splits = 5  # Common choice (5-fold CV)\n",
    "\n",
    "# Create Stratified K-Fold object\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"‚úÖ Stratified {n_splits}-Fold cross-validation is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T11:22:25.309736Z",
     "iopub.status.busy": "2025-07-26T11:22:25.309373Z",
     "iopub.status.idle": "2025-07-26T11:22:38.819938Z",
     "shell.execute_reply": "2025-07-26T11:22:38.818943Z",
     "shell.execute_reply.started": "2025-07-26T11:22:25.309709Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "‚úÖ Best parameters: {'model__subsample': 1.0, 'model__n_estimators': 300, 'model__max_depth': 3, 'model__learning_rate': 0.05, 'model__colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Fold 1 AUC: 0.9365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Fold 2 AUC: 0.9316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Fold 3 AUC: 0.9314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Fold 4 AUC: 0.9413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Fold 5 AUC: 0.9298\n",
      "\n",
      "üéØ Overall ROC AUC: 0.9339\n",
      "üìÅ submission_xgb_targetencoder.csv created.\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import clone\n",
    "\n",
    "# üì• Load the dataset\n",
    "train_data = pd.read_csv('/kaggle/input/customer-churn-ng-intern/train.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/customer-churn-ng-intern/test.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/customer-churn-ng-intern/sample_submission.csv')\n",
    "\n",
    "# üßΩ Feature setup\n",
    "drop_features = ['id', 'CustomerId', 'Surname']\n",
    "X_train = train_data.drop(columns=drop_features + ['Exited'])\n",
    "y_train = train_data['Exited']\n",
    "X_test = test_data.drop(columns=drop_features)\n",
    "\n",
    "# üîç Column types\n",
    "cat_cols = ['Geography', 'Gender']\n",
    "num_cols = [col for col in X_train.columns if col not in cat_cols]\n",
    "\n",
    "# üîÑ Preprocessing using TargetEncoder + StandardScaler\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', TargetEncoder(), cat_cols)\n",
    "])\n",
    "\n",
    "# üîß XGBoost Classifier\n",
    "xgb = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ‚öôÔ∏è Pipeline (used for hyperparameter tuning only)\n",
    "clf_pipeline = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', xgb)\n",
    "])\n",
    "\n",
    "# üîç Hyperparameter search space\n",
    "param_distributions = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__subsample': [0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# üîé RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    clf_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# üöÇ Fit for best params\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"‚úÖ Best parameters:\", random_search.best_params_)\n",
    "\n",
    "# üí° Final model and preprocessing\n",
    "best_model = random_search.best_estimator_.named_steps['model']\n",
    "\n",
    "# Fit preprocessor\n",
    "X_train_transformed = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# üîÅ Stratified K-Fold CV with early stopping\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(len(X_train))\n",
    "test_preds = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    X_tr, X_val = X_train_transformed[train_idx], X_train_transformed[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    model = clone(best_model)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=30,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    test_preds += model.predict_proba(X_test_transformed)[:, 1] / skf.n_splits\n",
    "\n",
    "    fold_score = roc_auc_score(y_val, oof_preds[val_idx])\n",
    "    print(f\"üìà Fold {fold+1} AUC: {fold_score:.4f}\")\n",
    "\n",
    "# üéØ Overall AUC\n",
    "print(f\"\\nüéØ Overall ROC AUC: {roc_auc_score(y_train, oof_preds):.4f}\")\n",
    "\n",
    "# üì§ Submission\n",
    "sample_sub['Exited'] = test_preds\n",
    "sample_sub.to_csv('submission_xgb_targetencoder.csv', index=False)\n",
    "print(\"üìÅ submission_xgb_targetencoder.csv created.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11682734,
     "sourceId": 98059,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
